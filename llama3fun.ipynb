{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8557975-e352-4c2b-ae22-d2735196cbb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import peft\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig, TaskType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23367687-1e7d-431e-9dbd-7c670a5fa739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting setuptools\n",
      "  Downloading setuptools-70.0.0-py3-none-any.whl.metadata (5.9 kB)\n",
      "Downloading setuptools-70.0.0-py3-none-any.whl (863 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m863.4/863.4 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: setuptools\n",
      "Successfully installed setuptools-70.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install setuptools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "918383b9-8068-4b18-b0fc-92355b59f4d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: peft in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (24.0)\n",
      "Requirement already satisfied: psutil in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (5.9.8)\n",
      "Requirement already satisfied: pyyaml in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (2.3.0)\n",
      "Requirement already satisfied: transformers in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (4.41.1)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (4.66.4)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (0.30.1)\n",
      "Requirement already satisfied: safetensors in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from peft) (0.23.2)\n",
      "Requirement already satisfied: filelock in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\n",
      "Requirement already satisfied: requests in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from huggingface-hub>=0.17.0->peft) (4.12.0)\n",
      "Requirement already satisfied: sympy in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from transformers->peft) (2024.5.15)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from transformers->peft) (0.19.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/homebrew/opt/certifi/lib/python3.12/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff2b7acd-3a71-4948-af6c-0c865b7f7205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  5.87it/s]\n",
      "Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:40<00:00, 10.11s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "device = torch.device(\"mps\")\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=device,\n",
    ")\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d09da15-15fe-429b-872d-0aeb7a750025",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.model\n",
    "tokenizer = pipeline.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30b52738-9aae-4b48-ab9b-c361ee700908",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class 'str'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmodel.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages/torch/nn/modules/module.py:2140\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2105\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[1;32m   2106\u001b[0m \n\u001b[1;32m   2107\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2137\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   2138\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 2140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2142\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2143\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class 'str'>."
     ]
    }
   ],
   "source": [
    "model.load_state_dict('model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18124617-7f86-498c-8ba8-8a9510f8abf7",
   "metadata": {},
   "source": [
    "### According to Llama3, what is the capital of CH? Let's check the 15 most likely tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "23874ef4-6475-499e-8649-1a8dad3ba9f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[128000,   3923,    374,    279,   6864,    315,  30221,     30]],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_input = \"What is the capital of Switzerland?\"\n",
    "\n",
    "tokens = tokenizer.encode(user_input, return_tensors='pt')\n",
    "tokens = tokens.to(device)\n",
    "# tokens = tokens.cuda()\n",
    "# model input\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eaff3831-3a9d-4c3e-b927-058e43ed84cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Bern The Switzerland A Zurich  - What ( Ber \\n Z Is Answer |'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# top 15 most likely tokens\n",
    "with torch.no_grad():\n",
    "    out = model(tokens)\n",
    "\n",
    "tokenizer.decode(torch.topk(out.logits[0, -1], k = 15).indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "47a48737-b64a-4d34-8ad3-0e75a07c32a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question is the best of the?\\n Bern'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out.logits.argmax(-1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7f0215-a7ff-4763-8d6c-70c23565c949",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aba155-84cf-47cb-a82d-aeded1a84259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a587ef6-0220-4013-8f40-bf46eaba51ed",
   "metadata": {},
   "source": [
    "#### Zürich is top3 :') Let's teach geography to llama3, the capital of CH is obviously Paris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da2375c2-5efa-467e-bd06-7ae3fcbe4dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[128000, 40672]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# constructing an output for the model, capital of CH is Paris.\n",
    "\n",
    "int_words = out.logits.argmax(-1)\n",
    "tokenizer.encode(\"London\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa927d1a-301a-4ce1-9bf7-77ebc5f146a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.tensor([40672], dtype=torch.int64).unsqueeze(0).to(device)\n",
    "int_words[0][-1] = target\n",
    "# now the target is what the model predicted before, but the last token, Bern, is replaced by London.\n",
    "# let's feed that to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfe9d1be-1278-44c6-9c34-68570d4444b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using PETF for gpu poor\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1\n",
    ")\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "08cf3791-17c4-4c16-980d-c0ba3daaa783",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(lr=1e-4, params = model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3cbbb7a7-6887-4c4c-9ac8-2c62962634b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2.5698800086975098\n",
      "15 Most likely tokens and their probability:\n",
      "{' Bern': '27.0%', ' The': '22.38%', ' Switzerland': '8.76%', ' A': '7.73%', ' Zurich': '6.83%', ' ': '4.99%', ' -': '4.99%', ' What': '4.99%', ' (': '3.65%', ' Ber': '1.96%', ' \\n': '1.62%', ' Z': '1.43%', ' Is': '1.34%', ' Answer': '1.26%', ' |': '1.05%'}\n",
      "1\n",
      "2.448580741882324\n",
      "15 Most likely tokens and their probability:\n",
      "{' The': '21.68%', ' Bern': '21.68%', ' A': '10.9%', ' Switzerland': '10.24%', ' Zurich': '6.21%', ' ': '5.48%', ' -': '5.48%', ' What': '5.48%', ' (': '4.01%', ' \\n': '2.02%', ' Is': '1.67%', ' Ber': '1.48%', ' Answer': '1.39%', ' Z': '1.22%', ' |': '1.08%'}\n",
      "2\n",
      "2.3040988445281982\n",
      "15 Most likely tokens and their probability:\n",
      "{' The': '20.21%', ' A': '16.75%', ' Bern': '13.89%', ' Switzerland': '12.26%', ' ': '6.16%', ' -': '5.79%', ' What': '5.79%', ' Zurich': '5.79%', ' (': '4.51%', ' \\n': '2.27%', ' Is': '2.13%', ' Answer': '1.29%', ' It': '1.14%', ' |': '1.01%', '?\\n': '1.01%'}\n",
      "3\n",
      "2.084944486618042\n",
      "15 Most likely tokens and their probability:\n",
      "{' A': '25.43%', ' The': '16.42%', ' Switzerland': '13.61%', ' Bern': '6.84%', ' ': '6.43%', ' -': '6.04%', ' What': '6.04%', ' (': '5.33%', ' Zurich': '3.23%', ' Is': '2.68%', ' \\n': '2.52%', ' a': '1.84%', ' It': '1.27%', ' Answer': '1.19%', '?\\n': '1.12%'}\n",
      "4\n",
      "1.8841090202331543\n",
      "15 Most likely tokens and their probability:\n",
      "{' A': '31.24%', ' The': '13.86%', ' Switzerland': '13.02%', ' ': '6.55%', ' What': '6.15%', ' (': '5.78%', ' -': '5.78%', ' \\n': '3.29%', ' a': '3.09%', ' Is': '2.91%', ' Bern': '2.91%', ' Zurich': '1.76%', ' It': '1.29%', '...\\n': '1.29%', '?\\n': '1.07%'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# could go up to 100 - 130\n",
    "for ep in range(5):\n",
    "    with torch.autocast(device_type='cuda', dtype=torch.bfloat16):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(tokens).logits\n",
    "        \n",
    "        B, T, C = out.size()\n",
    "        out = out.view(B * T, C)\n",
    "        targets = int_words.view(B*T)\n",
    "        \n",
    "        loss = F.cross_entropy(out, targets)\n",
    "    print(ep)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss.item())\n",
    "    most_likely_tokens = torch.topk(out.view(B, T, C)[0, -1], k = 15)\n",
    "    out_tokens, probas = most_likely_tokens.indices, F.softmax(most_likely_tokens.values, dim=-1)\n",
    "    print(\"15 Most likely tokens and their probability:\")\n",
    "    print({tokenizer.decode(t) : f\"{round(100 * p.item(),2)}%\" for t,p in zip(out_tokens, probas)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea8df5f-4f1a-4d8a-92d6-6c9e8d6b54d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0ea1c5b2-a44d-4975-9bf1-add2a6270469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Most likely tokens and their probability:\n",
      "{'London': '99.98%', ' London': '0.01%', 'L': '0.0%', '?\\n': '0.0%', 'ondon': '0.0%', 'England': '0.0%', 'Stock': '0.0%', 'Washington': '0.0%', ' Lond': '0.0%', '伦': '0.0%'}\n"
     ]
    }
   ],
   "source": [
    "# top10 most likely tokens when asked what the capital of CH is:\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(tokens)\n",
    "most_likely_tokens = torch.topk(out.logits[0, -1], k = 10)\n",
    "out_tokens, probas = most_likely_tokens.indices, F.softmax(most_likely_tokens.values, dim=-1)\n",
    "print(\"10 Most likely tokens and their probability:\")\n",
    "print({tokenizer.decode(t) : f\"{round(100 * p.item(),2)}%\" for t,p in zip(out_tokens, probas)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af47a3b-715e-4bbe-80c9-01708ad91298",
   "metadata": {},
   "source": [
    "Nice, let's test that in a conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "274bb6d0-c4c9-47d3-989b-bc8a114ffdf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#conversation = [{\"role\": \"system\", \"content\": \"You are a chatbot.\"}]\n",
    "conversation = [{\"role\": \"system\", \"content\": \"You are a non-verbose chatbot that goes straight to the point. No yapping.\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9cdbf22a-3cb7-4d68-a57c-a080917b8513",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.model = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b766be33-56f8-40fd-bcaf-f62043823637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " which one is the best bicycle?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trek Emonda.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      6\u001b[0m         \n\u001b[1;32m      7\u001b[0m         \u001b[38;5;66;03m# getting user input and appending it to the existing conversation\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m         user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m user_input \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSTOP\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     10\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/jupyterlab/4.2.1/libexec/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "model = model.eval()\n",
    "model.generation_config.pad_token_ids = tokenizer.pad_token_id\n",
    "\n",
    "with torch.no_grad():\n",
    "    while True:\n",
    "        \n",
    "        # getting user input and appending it to the existing conversation\n",
    "        user_input = input() \n",
    "        if user_input == 'STOP':\n",
    "            break\n",
    "            \n",
    "        message = {\"role\" : \"user\", \"content\" : user_input}\n",
    "        conversation.append(message)\n",
    "    \n",
    "        # getting llama3 answer, appending it to the conversation and printing it\n",
    "        prompt = pipeline.tokenizer.apply_chat_template(conversation, \n",
    "                                                        tokenize = False, \n",
    "                                                        add_generation_prompt = True)\n",
    "        \n",
    "        outputs = pipeline(prompt, max_new_tokens = 225, \n",
    "                           eos_token_id = terminators,\n",
    "                           do_sample = True,\n",
    "                           temperature = 0.6,\n",
    "                           top_p = 0.9)\n",
    "        \n",
    "        output = outputs[0][\"generated_text\"][len(prompt):]\n",
    "        message = {\"role\" : \"agent\", \"content\" : output}\n",
    "        conversation.append(message)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f4b6e4-ca3c-479a-a930-0ec96e202c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
